{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division,print_function\n",
    "\n",
    "import os, json\n",
    "import shutil\n",
    "from glob import glob\n",
    "import random\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla P100-PCIE-16GB (CNMeM is disabled, cuDNN not available)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import utils; reload(utils)\n",
    "from utils import plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import SGD, RMSprop, Adam, Nadam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create validation set and sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NB_ROOT = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_HOME_DIR = os.path.join(NB_ROOT, \"data/invasive-species-monitoring\")\n",
    "results_path = os.path.join('/mnt/data/invasive-species-monitoring', 'results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/ml/working/fastai-courses/deeplearning1/nbs/data/invasive-species-monitoring\n"
     ]
    }
   ],
   "source": [
    "%cd $DATA_HOME_DIR\n",
    "%mkdir -p valid\n",
    "%mkdir -p results\n",
    "%mkdir -p sample/train\n",
    "%mkdir -p sample/test/unknown\n",
    "%mkdir -p sample/valid\n",
    "%mkdir -p sample/results\n",
    "%mkdir -p test/unknown\n",
    "%mkdir -p /mnt/data/invasive-species-monitoring/results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_label_dirs(base_dir):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Create label directories thant can be recognized by Keras ImageDataGenerator.flow_from_directory    \n",
    "    labels = ['invasive', 'not_invasive']\n",
    "    for label in labels:\n",
    "        try:\n",
    "            os.makedirs(os.path.join(base_dir, label))\n",
    "        except OSError as e:\n",
    "            if e.errno != os.errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "create_label_dirs('train')\n",
    "create_label_dirs('valid')\n",
    "create_label_dirs('sample/train')\n",
    "create_label_dirs('sample/valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels_csv = pd.read_csv(\"train_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_images_by_dir(base_dir=\"train\"):\n",
    "    \"\"\"\n",
    "    Move images into their label directory so that they can be recognized by\n",
    "    ImageDataGenerator.flow_from_directory\n",
    "    \"\"\"\n",
    "    for _, row in train_labels_csv.iterrows():\n",
    "        image_name = \"{}.jpg\".format(row['name'])\n",
    "        src_path = os.path.join(base_dir, image_name)\n",
    "        if row['invasive'] == 1:\n",
    "            dst_path = os.path.join(base_dir, 'invasive', image_name)\n",
    "        else:\n",
    "            dst_path = \"train/not_invasive/{}\".format(image_name)\n",
    "        if os.path.exists(src_path):\n",
    "            shutil.move(src_path, dst_path)\n",
    "# label_images_by_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_valid_set(train_root='train', valid_root=\"valid\", valid_rate=0.1):\n",
    "    labels = ['invasive', 'not_invasive']\n",
    "    for label in labels:\n",
    "        train_label_dir = os.path.join(train_root, label)\n",
    "        valid_label_dir = os.path.join(valid_root, label)\n",
    "        files = os.listdir(train_label_dir)\n",
    "        for file in random.sample(files, k=int(len(files) * valid_rate)):\n",
    "            shutil.move(os.path.join(train_label_dir, file), valid_label_dir)\n",
    "# create_valid_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_sample_set(rate=0.01):\n",
    "    labels = ['invasive', 'not_invasive', 'unknown']\n",
    "    for dataset in ['train', 'valid', 'test']:\n",
    "        if dataset == \"valid\":\n",
    "            # use a higher smple rate for the validation dataset\n",
    "            # because they have fewer items\n",
    "            sample_rate = rate*5\n",
    "        else:\n",
    "            sample_rate = rate\n",
    "        for label in labels:\n",
    "            src_dir = os.path.join(dataset, label)\n",
    "            dst_dir = os.path.join('sample', dataset, label)\n",
    "            try:\n",
    "                files = os.listdir(src_dir)\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "            for file in random.sample(files, k=int(len(files) * sample_rate)):\n",
    "                shutil.copy(os.path.join(src_dir, file), dst_dir)\n",
    "\n",
    "# create_sample_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import applications\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    img_rows, img_cols, img_channel = 224, 224, 3\n",
    "    base_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(img_channel,img_rows, img_cols))\n",
    "    base_model\n",
    "    \n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    add_model = Sequential()\n",
    "    add_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "    add_model.add(Dense(256, activation='relu'))\n",
    "    add_model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_path = DATA_HOME_DIR\n",
    "# _path = DATA_HOME_DIR + '/sample' # Only for sample tests!\n",
    "test_path = os.path.join(DATA_HOME_DIR, 'test')\n",
    "train_path = os.path.join(_path, 'train')\n",
    "valid_path = os.path.join(_path, 'valid')\n",
    "test_path = os.path.join(_path, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(path, gen=image.ImageDataGenerator(), shuffle=True, batch_size=8, target_size=(224,224), class_mode='categorical'):\n",
    "        \"\"\"\n",
    "            Takes the path to a directory, and generates batches of augmented/normalized data. Yields batches indefinitely, in an infinite loop.\n",
    "\n",
    "            See Keras documentation: https://keras.io/preprocessing/image/\n",
    "        \"\"\"\n",
    "        # 224x224 is the image size used by ImageNet\n",
    "        return gen.flow_from_directory(path, target_size=target_size,\n",
    "                class_mode=class_mode, shuffle=shuffle, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2067 images belonging to 2 classes.\n",
      "Found 228 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "! cd $NB_ROOT\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "trans_gen = image.ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1, height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "# target_size = (600, 450)\n",
    "target_size = (224, 224) \n",
    "train_batches = get_batches(train_path, gen=trans_gen, batch_size=BATCH_SIZE, target_size=target_size)\n",
    "valid_batches = get_batches(valid_path, batch_size=BATCH_SIZE*2, target_size=target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_model(model, no_of_epochs = 10):\n",
    "    latest_weights_filename = None\n",
    "    for epoch in range(no_of_epochs):\n",
    "        print(\"Running epoch: %d\" % epoch)\n",
    "        model.fit_generator(\n",
    "            train_batches,\n",
    "            epochs=1,\n",
    "            steps_per_epoch=train_batches.samples//BATCH_SIZE,\n",
    "            validation_data=valid_batches, validation_steps=valid_batches.samples//BATCH_SIZE,\n",
    "        )\n",
    "        latest_weights_filename = 'ft%d.h5' % epoch\n",
    "        model.save_weights(os.path.join(results_path, latest_weights_filename))\n",
    "        print(\"Completed %s fit operations\" % epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch: 0\n",
      "Epoch 1/1\n",
      "65/64 [==============================] - 88s 1s/step - loss: 2.0113 - acc: 0.8210 - val_loss: 1.4243 - val_acc: 0.8904\n",
      "Completed 0 fit operations\n",
      "Running epoch: 1\n",
      "Epoch 1/1\n",
      "65/64 [==============================] - 88s 1s/step - loss: 1.5846 - acc: 0.8580 - val_loss: 0.7107 - val_acc: 0.9342\n",
      "Completed 1 fit operations\n",
      "Running epoch: 2\n",
      "Epoch 1/1\n",
      "65/64 [==============================] - 88s 1s/step - loss: 1.2360 - acc: 0.8899 - val_loss: 0.9827 - val_acc: 0.8991\n",
      "Completed 2 fit operations\n",
      "Running epoch: 3\n",
      "Epoch 1/1\n",
      "65/64 [==============================] - 88s 1s/step - loss: 0.9124 - acc: 0.9022 - val_loss: 0.4549 - val_acc: 0.9386\n",
      "Completed 3 fit operations\n",
      "Running epoch: 4\n",
      "Epoch 1/1\n",
      "65/64 [==============================] - 88s 1s/step - loss: 0.6799 - acc: 0.9060 - val_loss: 0.4546 - val_acc: 0.9298\n",
      "Completed 4 fit operations\n",
      "Running epoch: 5\n",
      "Epoch 1/1\n",
      "65/64 [==============================] - 88s 1s/step - loss: 0.3971 - acc: 0.9191 - val_loss: 0.2509 - val_acc: 0.9298\n",
      "Completed 5 fit operations\n",
      "Running epoch: 6\n",
      "Epoch 1/1\n",
      "65/64 [==============================] - 88s 1s/step - loss: 0.2775 - acc: 0.9075 - val_loss: 0.2550 - val_acc: 0.9167\n",
      "Completed 6 fit operations\n",
      "Running epoch: 7\n",
      "Epoch 1/1\n",
      "65/64 [==============================] - 88s 1s/step - loss: 0.2201 - acc: 0.9213 - val_loss: 0.2330 - val_acc: 0.9254\n",
      "Completed 7 fit operations\n",
      "Running epoch: 8\n",
      "Epoch 1/1\n",
      "65/64 [==============================] - 88s 1s/step - loss: 0.1886 - acc: 0.9261 - val_loss: 0.2398 - val_acc: 0.9342\n",
      "Completed 8 fit operations\n",
      "Running epoch: 9\n",
      "Epoch 1/1\n",
      "65/64 [==============================] - 88s 1s/step - loss: 0.1856 - acc: 0.9349 - val_loss: 0.2293 - val_acc: 0.9342\n",
      "Completed 9 fit operations\n",
      "Running epoch: 10\n",
      "Epoch 1/1\n",
      "65/64 [==============================] - 88s 1s/step - loss: 0.1974 - acc: 0.9198 - val_loss: 0.2312 - val_acc: 0.9254\n",
      "Completed 10 fit operations\n",
      "Running epoch: 11\n",
      "Epoch 1/1\n",
      "65/64 [==============================] - 88s 1s/step - loss: 0.1994 - acc: 0.9229 - val_loss: 0.2063 - val_acc: 0.9254\n",
      "Completed 11 fit operations\n",
      "Running epoch: 12\n",
      "Epoch 1/1\n",
      "65/64 [==============================] - 88s 1s/step - loss: 0.1691 - acc: 0.9341 - val_loss: 0.2292 - val_acc: 0.9298\n",
      "Completed 12 fit operations\n",
      "Running epoch: 13\n",
      "Epoch 1/1\n",
      "65/64 [==============================] - 88s 1s/step - loss: 0.1522 - acc: 0.9311 - val_loss: 0.2077 - val_acc: 0.9254\n",
      "Completed 13 fit operations\n",
      "Running epoch: 14\n",
      "Epoch 1/1\n",
      "65/64 [==============================] - 88s 1s/step - loss: 0.1761 - acc: 0.9268 - val_loss: 0.1949 - val_acc: 0.9298\n",
      "Completed 14 fit operations\n",
      "Running epoch: 15\n",
      "Epoch 1/1\n",
      "65/64 [==============================] - 88s 1s/step - loss: 0.1537 - acc: 0.9442 - val_loss: 0.2112 - val_acc: 0.9342\n",
      "Completed 15 fit operations\n",
      "Running epoch: 16\n",
      "Epoch 1/1\n",
      "65/64 [==============================] - 88s 1s/step - loss: 0.1599 - acc: 0.9388 - val_loss: 0.1949 - val_acc: 0.9342\n",
      "Completed 16 fit operations\n",
      "Running epoch: 17\n",
      "Epoch 1/1\n",
      "65/64 [==============================] - 88s 1s/step - loss: 0.1451 - acc: 0.9401 - val_loss: 0.1850 - val_acc: 0.9342\n",
      "Completed 17 fit operations\n",
      "Running epoch: 18\n",
      "Epoch 1/1\n",
      "65/64 [==============================] - 88s 1s/step - loss: 0.1509 - acc: 0.9391 - val_loss: 0.2000 - val_acc: 0.9298\n",
      "Completed 18 fit operations\n",
      "Running epoch: 19\n",
      "Epoch 1/1\n",
      "65/64 [==============================] - 88s 1s/step - loss: 0.1359 - acc: 0.9468 - val_loss: 0.1992 - val_acc: 0.9211\n",
      "Completed 19 fit operations\n"
     ]
    }
   ],
   "source": [
    "fit_model(model, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit_model(model, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1531 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_batches = get_batches(\n",
    "    test_path, batch_size=BATCH_SIZE * 2, target_size=target_size, shuffle=False, class_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(model, test_batches):\n",
    "    return model.predict_generator(test_batches, test_batches.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict(model, test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sbm = pd.DataFrame(preds, columns=[\"invasive\",\"not invasive\"])\n",
    "sbm['name'] = [int(f.replace('unknown/', '').replace('.jpg', '')) for f in test_batches.filenames]\n",
    "sbm = sbm.set_index(['name'])\n",
    "sbm = sbm.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sbm.to_csv('submission.csv', columns=['invasive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96203\r\n"
     ]
    }
   ],
   "source": [
    "!kg submit -c invasive-species-monitoring submission.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
